{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "console.log(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HfInference } from \"@huggingface/inference\";\n",
    "import dotenv from \"dotenv\";\n",
    "\n",
    "// Load environment variables from .env file\n",
    "dotenv.config();\n",
    "\n",
    "// Ensure the API key is provided\n",
    "const apiKey = process.env.HUGGING_FACE_API_KEY;\n",
    "if (!apiKey) {\n",
    "    throw new Error(\"Please set your HUGGING_FACE_API_KEY in the .env file\");\n",
    "}\n",
    "\n",
    "// Create an instance of the HfInference client\n",
    "const inference = new HfInference(apiKey as string);\n",
    "\n",
    "async function classifyTexts() {\n",
    "    try {\n",
    "        // Define multiple inputs\n",
    "        const inputs = [\n",
    "            \"Today is a great day\",\n",
    "            \"doggos\",\n",
    "        ];\n",
    "\n",
    "        const results = await inference.textClassification({\n",
    "            model: \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            inputs: inputs,\n",
    "        });\n",
    "        \n",
    "        // Log the classification results\n",
    "        console.log(\"Classification Results:\", results);\n",
    "    } catch (error) {\n",
    "        console.error(\"Error during inference:\", error);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Execute the classification function\n",
    "classifyTexts(); // Note: \"classifyText\" changed to \"classifyTexts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HfInference } from \"@huggingface/inference\";\n",
    "import dotenv from \"dotenv\";\n",
    "\n",
    "// Load environment variables from .env file\n",
    "dotenv.config();\n",
    "// Function to generate text using the specified Hugging Face model\n",
    "async function askQuestion(question: string) {\n",
    "    const hf = new HfInference(process.env.HUGGING_FACE_API_KEY as string); // Use the API key from the environment variable\n",
    "\n",
    "    // Use the \"cognitivecomputations/WizardLM-7B-Uncensored\" model for text generation\n",
    "  \n",
    "\n",
    "\n",
    "    try {\n",
    "        const result = await hf.textGeneration({\n",
    "            model: \"Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF\",\n",
    "            inputs: question,\n",
    "        });\n",
    "        \n",
    "        console.log(result);\n",
    "    } catch (error) {\n",
    "        console.error(\"Error during inference:\", error);\n",
    "    }\n",
    "}\n",
    "\n",
    "async function main() {\n",
    "    await askQuestion(\"What is the capital of France?\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async function main() {\n",
    "    await askQuestion(\"What is the capital of France?\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HfInference } from \"@huggingface/inference\";\n",
    "import dotenv from \"dotenv\";\n",
    "\n",
    "dotenv.config();\n",
    "\n",
    "const inference = new HfInference(process.env.HUGGING_FACE_API_KEY);\n",
    "\n",
    "const result = await inference.textClassification({\n",
    "    model: \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    inputs: \"Today is a great day\",\n",
    "});\n",
    "\n",
    "console.log(result);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for await (const chunk of inference.chatCompletionStream({\n",
    "\tmodel: \"google/gemma-2-2b-it\",\n",
    "\tmessages: [{ role: \"user\", content: \"What is the capital of France?\" }],\n",
    "\tmax_tokens: 50,\n",
    "})) {\n",
    "\tprocess.stdout.write(chunk.choices[0]?.delta?.content || \"\");\n",
    "    console.log(chunk);\n",
    "}\n",
    "//enhanceaiteam/Flux-uncensored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { \u001b[36m<pending>\u001b[39m }"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\":\"tell me about yourself \\n\\nPlease feel free to mention:\\n* Your passion\\n* Your skills \\n* What you enjoy doing\\n\\nLet's connect!\\n\"}\n"
     ]
    }
   ],
   "source": [
    "import { HfInference } from \"@huggingface/inference\";\n",
    "import dotenv from \"dotenv\";\n",
    "\n",
    "// Load environment variables from .env file\n",
    "dotenv.config();\n",
    "\n",
    "// Initialize the Hugging Face Inference with the API key\n",
    "const inference = new HfInference(process.env.HUGGING_FACE_API_KEY);\n",
    "\n",
    "// Function to query the model\n",
    "const queryModel = async (data) => {\n",
    "    try {\n",
    "        // Send a request to the text generation model\n",
    "        const response = await inference.textGeneration(data);\n",
    "        return response;\n",
    "    } catch (error) {\n",
    "        console.error(\"Error during inference:\", error);\n",
    "        throw error; // Re-throw the error for further handling if needed\n",
    "    }\n",
    "};\n",
    "\n",
    "// Example usage\n",
    "const inputArgs = {\n",
    "    inputs: \"tell me about yourself \" \n",
    "};\n",
    "\n",
    "// Call the query function and handle the response\n",
    "queryModel(inputArgs)\n",
    "    .then((response) => {\n",
    "        console.log(JSON.stringify(response));\n",
    "    })\n",
    "    .catch((error) => {\n",
    "        console.error(\"Failed to get response:\", error);\n",
    "    });\n",
    "\n",
    "// Streaming example\n",
    "const streamChatCompletion = async () => {\n",
    "    let out = \"\";\n",
    "    for await (const chunk of inference.chatCompletionStream({\n",
    "        model: \"enhanceaiteam/Flux-uncensored\",\n",
    "        messages: [{ role: \"user\", content: \"what model are you\" }],\n",
    "        max_tokens: 100,\n",
    "        temperature: 0.1,\n",
    "    })) {\n",
    "        if (chunk.choices && chunk.choices.length > 0) {\n",
    "            out += chunk.choices[0].delta.content;\n",
    "        }\n",
    "    }\n",
    "    console.log(\"Streaming Output:\", out);\n",
    "};\n",
    "\n",
    "// Call the streaming function\n",
    "streamChatCompletion();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
